# Narrative Consistency Checker

**Clean, Production-Ready System** for verifying character backstory consistency against novel text using LLM-powered claim extraction and verification.

## ğŸ¯ Quick Start

### 1. Install Dependencies

```bash
pip install -r requirements.txt
```

### 2. Configure API Key

Set your Groq API key (recommended for speed and reliability):

**Windows PowerShell:**
```powershell
$env:GROQ_API_KEY = "your-groq-api-key"
```

**Linux/Mac:**
```bash
export GROQ_API_KEY="your-groq-api-key"
```

**Alternative Providers:** OpenRouter, Gemini (configure in `src/config.py`)

### 3. Run Testing

Navigate to the src directory and run:

```bash
cd src
python run_afk_mode.py
```

The system will:
- âœ… Automatically test all training examples
- âœ… Save progress continuously (can resume anytime)
- âœ… Handle API rate limits automatically
- âœ… Generate `submission.csv` for test predictions when complete

## ğŸ“ Clean Project Structure

```
kharagpur_hackathon/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config.py              # Unified configuration (LLM, paths, settings)
â”‚   â”œâ”€â”€ data_loader.py         # Data loading utilities
â”‚   â”œâ”€â”€ claim_decomposer.py    # LLM-based claim extraction
â”‚   â”œâ”€â”€ consistency_checker.py # LLM-based claim verification
â”‚   â”œâ”€â”€ evidence_retriever.py  # Evidence search from novel chunks
â”‚   â”œâ”€â”€ novel_ingester.py      # Novel chunking and processing
â”‚   â”œâ”€â”€ master_pipeline.py     # Main orchestration pipeline
â”‚   â”œâ”€â”€ auto_test_loop.py      # Autonomous testing with resume
â”‚   â”œâ”€â”€ run_afk_mode.py        # Runner script (START HERE)
â”‚   â”œâ”€â”€ cache_manager.py       # SQLite caching for API responses
â”‚   â””â”€â”€ smart_fallback.py      # Pattern-based fallback extraction
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ train.csv              # Training backstories with labels
â”‚   â”œâ”€â”€ test.csv               # Test backstories (predict these)
â”‚   â””â”€â”€ novels/                # Full novel texts
â”‚
â”œâ”€â”€ results/                   # Output predictions
â”œâ”€â”€ logs/                      # Auto-test progress logs
â”œâ”€â”€ cache/                     # SQLite caches (auto-created)
â”œâ”€â”€ requirements.txt           # Python dependencies
â””â”€â”€ README.md                  # This file
```

## ğŸš€ Features

### Core Capabilities
- **Multi-Provider LLM Support**: Groq, OpenRouter, Gemini
- **Smart Caching**: SQLite-based caching to avoid redundant API calls
- **Robust Error Handling**: Automatic retries, rate limit management
- **Resume Capability**: Saves progress continuously, can stop and restart anytime
- **Clean Code**: Refactored, readable, well-documented

### Pipeline Stages
1. **Claim Decomposition**: Extracts verifiable claims from backstory text
2. **Novel Ingestion**: Chunks novels into searchable segments
3. **Evidence Retrieval**: Finds relevant passages mentioning the character
4. **Consistency Verification**: LLM judges if claims match evidence
5. **Aggregation**: Combines multiple claim verdicts into final prediction

## âš™ï¸ Configuration

Edit `src/config.py` to customize:

```python
# LLM Provider
LLM_PROVIDER = "groq"  # Options: "groq", "openrouter", "gemini"

# Provider-specific settings (timeouts, delays, models)
LLM_CONFIG = {...}

# Claim extraction
MAX_CLAIMS_PER_BACKSTORY = 15
MIN_CLAIM_CONFIDENCE = 0.7

# Evidence retrieval
TOP_K_EVIDENCE = 10

# Testing
TEST_BATCH_SIZE = 5
AUTO_WAIT_ON_RATE_LIMIT = True
```

## ğŸ“Š Output

### Training Progress
Logged to `logs/auto_test_results.txt`:
```
Story 1: Pred=1, Actual=1.0, Conf=0.85, Rationale=2/3 claims supported...
Story 2: Pred=0, Actual=0.0, Conf=0.92, Rationale=4/4 claims contradicted...
```

### Test Predictions
Generated as `results/submission.csv`:
```csv
id,label
95,0
136,1
...
```

## ğŸ› ï¸ Development

### Adding New LLM Providers

1. Add API configuration to `config.py`:
```python
LLM_CONFIG["new_provider"] = {
    "model": "model-name",
    "api_key": os.getenv("NEW_PROVIDER_KEY"),
    ...
}
```

2. Implement API call in `claim_decomposer.py` and `consistency_checker.py`

### Adjusting Verification Logic

Edit `master_pipeline.py` â†’ `_aggregate_verifications()` to change how multiple claims are combined into final prediction.

## ğŸ› Troubleshooting

### API Rate Limits
- System automatically waits 12 hours when hitting rate limits
- Reduce `TEST_BATCH_SIZE` in config.py for stricter limits
- Increase delays in `LLM_CONFIG` for each provider

### Low Accuracy
- Tune `MAX_CLAIMS_PER_BACKSTORY` (fewer = more focused)
- Adjust `TOP_K_EVIDENCE` (more evidence = better context)
- Modify aggregation weights in `_aggregate_verifications()`

### Memory Issues
- Novel chunks are cached - clear `cache/` folder if needed
- Reduce chunk size in `novel_ingester.py`

## ğŸ“ˆ Performance

- **Caching**: ~80% of claims reuse cached extractions on retry
- **Speed**: ~30-60 seconds per story (depending on LLM provider)
- **Accuracy**: Varies by model (70b models perform better)

## ğŸ§¹ Recently Cleaned

Removed unnecessary files:
- âŒ `test_ollama_setup.py` - Ollama is not used
- âŒ `test_api.py`, `test_quick.py` - Old test scripts
- âŒ `setup.py`, `run_predictions.py` - Replaced by clean pipeline
- âŒ `list_models.py` - Utility not needed
- âŒ `run.bat`, `run.sh` - Use Python directly

**Current Structure**: Clean, minimal, production-ready âœ…

## ğŸ“ License

MIT License - See LICENSE file for details
